### Постановка задачи

Проводится эксперимент нахождения решения задачи Дирихле с заданной точностью.

Основная задача заключается в построении алгоритма 11.6  (Блочный подход к методу волновой обработки данных) из книги В. П. Гергеля “Высокопроизводительные вычисления для
многоядерных многопроцессорных систем” (2010) и исследовании эффктивности его распараллеливания при помощи библиотеки OpenMP.

Хотим сравнить результат ускорения за счет параллельности с результатами, описанными в книге. По таблице 11.3, применение алгоритма 11.6 при 8 потоках ускоряет нахождение решения в **4.52** раза.

### Тестовые данные

В качестве начальных данных задачи Дирихле были взяты следующие функции:

$$f(x, y) = 100*x - 1/(y + 0.01) + 1000*(y^2 +  3*x)$$

$$g(x, y) = 5*x + 10*y$$

Запуски выполнялись на процессоре: 12th Gen Intel(R) Core(TM) i5-12450H

Измерения проводились на сетках разного размера, с разным числом потоков и разными размерами блоков. На каждый инстанс было проведено 5 замеров.


### Результаты измерений

| N | Threads | Time, sec $$(mean \pm 2*std)$$ | Block size | Acceleration |
| - | - | - | - | - |
| 300 | 1 | $$8.38 \pm 0.7$$ | 1 | 1.0 |
| 300 | 4 | $$7.2 \pm 0.03$$ | 1 | 1.16 |
| 300 | 8 | $$9.05 \pm 0.08$$ | 1 | 0.93 |
| 300 | 1 | $$5.15 \pm 0.05$$ | 32 | 1.0 |
| 300 | 4 | $$2.68 \pm 0.05$$ | 32 | 1.92 |
| 300 | 8 | $$2.77 \pm 0.07$$ | 32 | 1.86 |
| 300 | 1 | $$5.18 \pm 0.05$$ | 64 | 1.0 |
| 300 | 4 | $$2.87 \pm 0.03$$ | 64 | 1.8 |
| 300 | 8 | $$3.45 \pm 0.13$$ | 64 | 1.5 |

Среднее улучшение при 8 потоках составило:  **1.43** 

| N | Threads | Time, sec $$(mean \pm 2*std)$$ | Block size | Acceleration |
| - | - | - | - | - |
| 500 | 1 | $$13.69 \pm 1.76$$ | 1 | 1.0 |
| 500 | 4 | $$6.99 \pm 0.12$$ | 1 | 1.96 |
| 500 | 8 | $$7.79 \pm 0.26$$ | 1 | 1.76 |
| 500 | 1 | $$6.76 \pm 0.08$$ | 32 | 1.0 |
| 500 | 4 | $$2.5 \pm 0.07$$ | 32 | 2.7 |
| 500 | 8 | $$2.45 \pm 0.03$$ | 32 | 2.76 |
| 500 | 1 | $$6.81 \pm 0.26$$ | 64 | 1.0 |
| 500 | 4 | $$2.81 \pm 0.07$$ | 64 | 2.42 |
| 500 | 8 | $$2.85 \pm 0.08$$ | 64 | 2.39 |

Среднее улучшение при 8 потоках составило:  **2.33** 

| N | Threads | Time, sec $$(mean \pm 2*std)$$ | Block size | Acceleration |
| - | - | - | - | - |
| 1000 | 1 | $$26.99 \pm 0.37$$ | 1 | 1.0 |
| 1000 | 4 | $$7.49 \pm 0.07$$ | 1 | 3.6 |
| 1000 | 8 | $$6.48 \pm 0.27$$ | 1 | 4.17 |
| 1000 | 1 | $$9.44 \pm 0.14$$ | 32 | 1.0 |
| 1000 | 4 | $$2.61 \pm 0.13$$ | 32 | 3.62 |
| 1000 | 8 | $$1.95 \pm 0.02$$ | 32 | 4.84 |
| 1000 | 1 | $$7.7 \pm 0.11$$ | 64 | 1.0 |
| 1000 | 4 | $$2.55 \pm 0.1$$ | 64 | 3.02 |
| 1000 | 8 | $$2.18 \pm 0.02$$ | 64 | 3.53 |

Среднее улучшение при 8 потоках составило:  **4.18** 

### Выводы

1) Улучшение производительности становится заметно с ростом размера сетки. Это связано с тем, что при маленьких размерах сетки размер блочной матрицы тоже получаетя маленьким (особенно при больших размерах блока). Это приводит к тому, что диагоналей для распараллеливания на меньших сетках просто меньше, из-за чего наблюдаем меньшее ускорение

2) Добавление обработки по блокам улучшает производительность независимо от количества используемых процессов. В моем случае наиболее эффективным оказались блоки размера 32x32

3) На самой большой сетке улучшение составило **4.16** раз, что похоже на табличные результаты из книги